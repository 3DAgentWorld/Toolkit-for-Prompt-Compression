# About datasets

Here you can find some detail information about each dataset. For each dataset, there is a link connected to its original GitHub repository.

## Reference

1. Suzgun, Mirac et al. “Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them.” Annual Meeting of the Association for Computational Linguistics (2022).
2. Bai, Yushi et al. “LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding.” ArXiv abs/2308.14508 (2023): n. pag.
3. Li, Yucheng et al. “Compressing Context to Enhance Inference Efficiency of Large Language Models.” Conference on Empirical Methods in Natural Language Processing (2023).
4. Cobbe, Karl et al. “Training Verifiers to Solve Math Word Problems.” ArXiv abs/2110.14168 (2021): n. pag.
5. Ghalandari, Demian Gholipour et al. “Efficient Unsupervised Sentence Compression by Fine-tuning Transformers with Reinforcement Learning.” ArXiv abs/2205.08221 (2022): n. pag.
